{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTE8Bunr1-bv"
      },
      "source": [
        "---\n",
        "# Cairo University Faculty of Engineering\n",
        "## Deep Learning\n",
        "## Assignment 1\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UePv0I-R1-bw"
      },
      "source": [
        "Please write your full name here\n",
        "- **Name** : \"-----------\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwJv1hq41-bw"
      },
      "source": [
        "## Table of Contents\n",
        "- [Part1: Tensorflow and Python](#1)\n",
        "    - [1.1 - Sigmoid function, tf.exp()](#1-1)\n",
        "    - [1.2 - Sigmoid gradient](#1-2)\n",
        "    - [1.3 - Reshaping arrays](#1-3)\n",
        "    - [1.4 - Normalizing rows](#1-4)\n",
        "        - [normalize_rows](#1-4-1)\n",
        "        - [softmax](#1-4-2)\n",
        "    - [2 - Vectorization](#2)\n",
        "        - [2.1 - Implement the L1 and L2 loss functions](#2-1)\n",
        "            - [L1](#2-1-1)\n",
        "            - [L2](#2-1-2)\n",
        "- [Part2: TensorFlow](#3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)\n",
        "random.seed(1234)"
      ],
      "metadata": {
        "id": "tjYkp5x7PGve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "k7IMuXMK1-bw"
      },
      "source": [
        "<a name='1'></a>\n",
        "# Part1: Tensorflow and Python\n",
        "## **Instructions:**\n",
        "\n",
        "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
        "- After coding your function, run the cell right below it to check if your result is correct.\n",
        "- **Use tensorflow in all your codes unless stated otherwise** ⏰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7liYtEjP1-bx"
      },
      "source": [
        "**You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "W6nNzL7I1-bx"
      },
      "source": [
        "<a name='1-1'></a>\n",
        "## 1 - Building basic functions with tensorflow ##\n",
        "\n",
        "### 1.1 - Sigmoid function ###\n",
        "\n",
        "**Exercise**: Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function.\n",
        "\n",
        "**Reminder**:\n",
        "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n",
        "\n",
        "<img src=\"https://i.ibb.co/4fw1Qzk/sigmoid.png\" alt=\"sigmoid\" border=\"0\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWuqKjty1-bx"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: basic_sigmoid\n",
        "\n",
        "def basic_sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute sigmoid of x.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA_0tKD01-by"
      },
      "outputs": [],
      "source": [
        "basic_sigmoid(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwPZw3p_1-by"
      },
      "outputs": [],
      "source": [
        "### One reason why we use \"tf\" instead of \"math\" in Deep Learning ###\n",
        "x = [1, 2, 3]\n",
        "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPhOI211-by"
      },
      "source": [
        "|**Exercise**: Implement the sigmoid function using TENSORFLOW.\n",
        "\n",
        "**Instructions**: x could now be either a real number, a vector, or a matrix.\n",
        "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
        "    x_1  \\\\\n",
        "    x_2  \\\\\n",
        "    ...  \\\\\n",
        "    x_n  \\\\\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
        "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
        "    ...  \\\\\n",
        "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
        "\\end{pmatrix}\\tag{1} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA3Gnc451-by"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: sigmoid\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y4u2Bn01-by"
      },
      "outputs": [],
      "source": [
        "x = np.array([1, 2, 3], dtype=float)\n",
        "sigmoid(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5nOJFc91-bz"
      },
      "source": [
        "<a name='1-2'></a>\n",
        "### 1.2 - Sigmoid gradient\n",
        "\n",
        "As you've seen, you will need to compute gradients to optimize loss functions. Let's calculate the gradient of the sigmoid function.\n",
        "\n",
        "**Exercise**: Calculate the gradient/derivative of the sigmoid function using LaTeX. SHOW YOUR WORK\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nD4gCPYV3pBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's code your first gradient function.\n",
        "\n",
        "**Exercise**: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. Use the formula you calculated in the previous exercise"
      ],
      "metadata": {
        "id": "7cDwqqMd3GBt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwUzHJRv1-bz"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: sigmoid_derivative\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
        "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
        "\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array\n",
        "\n",
        "    Return:\n",
        "    ds -- Your computed gradient.\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuQLMe_A1-bz"
      },
      "outputs": [],
      "source": [
        "x = np.array([1, 2, 3], dtype=float)\n",
        "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOjWEvJf1-bz"
      },
      "source": [
        "<a name='1-3'></a>\n",
        "### 1.3 - Reshaping arrays ###\n",
        "\n",
        "Two common functions used in deep learning are [tf.shape] and [tf.reshape()].\n",
        "- X.shape is used to get the shape (dimension) of a matrix/vector X.\n",
        "- X.reshape(...) is used to reshape X into some other dimension.\n",
        "\n",
        "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/2PVNWKL/image2vector-kiank.png\" alt=\"image2vector-kiank\" border=\"0\">\n",
        "\n",
        "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1).\n",
        "\n",
        "- Please don't hardcode the dimensions of image as a constant. Instead look up the quantities you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHVRsKNE1-bz"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: image2vector\n",
        "def image2vector(image):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    image -- a numpy array of shape (length, height, depth)\n",
        "\n",
        "    Returns:\n",
        "    v -- a vector of shape (length*height*depth, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0ZKpomX1-bz"
      },
      "outputs": [],
      "source": [
        "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
        "image = np.array([[[ 0.67826139,  0.29380381],\n",
        "        [ 0.90714982,  0.52835647],\n",
        "        [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "       [[ 0.92814219,  0.96677647],\n",
        "        [ 0.85304703,  0.52351845],\n",
        "        [ 0.19981397,  0.27417313]],\n",
        "\n",
        "       [[ 0.60659855,  0.00533165],\n",
        "        [ 0.10820313,  0.49978937],\n",
        "        [ 0.34144279,  0.94630077]]])\n",
        "\n",
        "print (\"image2vector(image) = \" + str(image2vector(image)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUMWV8V1-bz"
      },
      "source": [
        "<a name='1-4'></a>\n",
        "### 1.4 - Normalizing Rows ####\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copY1O9I1-b0"
      },
      "source": [
        "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
        "\n",
        "For example, if\n",
        "$$x = \\begin{bmatrix}\n",
        "        0 & 3 & 4 \\\\\n",
        "        2 & 6 & 4 \\\\\n",
        "\\end{bmatrix}\\tag{3}$$\n",
        "then\n",
        "$$\\| x\\| =  \\begin{bmatrix}\n",
        "    5 \\\\\n",
        "    \\sqrt{56} \\\\\n",
        "\\end{bmatrix}\\tag{4} $$\n",
        "and\n",
        "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
        "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
        "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
        "\\end{bmatrix}\\tag{5}$$\n",
        "\n",
        "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting.\n",
        "\n",
        "HINTS:\n",
        "- `keepdims`\n",
        "- tf.norm has another parameter `ord` where we specify the type of normalization to be done (in the exercise below you'll do 2-norm).\n",
        "\n",
        "<a name='1-4-1'></a>\n",
        "#### 1.4.1 - Normalize_rows\n",
        "Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hWhZrER1-b0"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: normalize_rows\n",
        "\n",
        "def normalize_rows(x):\n",
        "    \"\"\"\n",
        "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
        "\n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (n, m)\n",
        "\n",
        "    Returns:\n",
        "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
        "    \"\"\"\n",
        "\n",
        "    #(≈ 2 lines of code)\n",
        "    # Compute x_norm as the norm 2 of x. Use tf.norm\n",
        "    # x_norm =\n",
        "    # Divide x by its norm.\n",
        "    # x =\n",
        "    # YOUR CODE STARTS HERE\n",
        "\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1ub2W7D1-b0"
      },
      "outputs": [],
      "source": [
        "x = np.array([[0, 3, 4],\n",
        "              [1, 6, 4]], dtype=float)\n",
        "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfwfV3_u1-b0"
      },
      "source": [
        "**Note**:\n",
        "In normalize_rows(), you can try to print the shapes of x_norm and x, and then rerun the assessment. You'll find out that they have different shapes. This is normal given that x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column. So how did it work when you divided x by x_norm? This is called broadcasting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "MqoNar2c1-b0"
      },
      "source": [
        "<a name='1-4-2'></a>\n",
        "#### 1.4.2 - Softmax function ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw-PFroI1-b0"
      },
      "source": [
        "**Exercise**: Implement a softmax function using tensorflow. You can think of softmax as a normalizing function (makes the sum of features of a sample to equal 1) used when your algorithm needs to classify two or more classes. You will learn more about softmax later in the course.\n",
        "\n",
        "**Instructions**:\n",
        "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
        "    x_1  &&\n",
        "    x_2 &&\n",
        "    ...  &&\n",
        "    x_n  \n",
        "\\end{bmatrix}) = \\begin{bmatrix}\n",
        "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
        "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
        "    ...  &&\n",
        "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}}\n",
        "\\end{bmatrix} $\n",
        "\n",
        "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
        "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
        "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
        "\\end{bmatrix} = \\begin{bmatrix}\n",
        "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
        "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
        "\\end{bmatrix} = \\begin{pmatrix}\n",
        "    softmax\\text{(first row of x)}  \\\\\n",
        "    softmax\\text{(second row of x)} \\\\\n",
        "    ...  \\\\\n",
        "    softmax\\text{(last row of x)} \\\\\n",
        "\\end{pmatrix} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujjZyZZ31-b0"
      },
      "source": [
        "**NOTE**\n",
        "\n",
        "\"m\" is used to represent the \"number of training examples\".\n",
        "Softmax should be performed for all features of each training example, so softmax would be performed on the rows.\n",
        "\n",
        "$m$ is the number of rows and $n$ is the number of columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_TSH9a51-b0"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: softmax\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Calculates the softmax for each row of the input x.\n",
        "\n",
        "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
        "\n",
        "    Argument:\n",
        "    x -- A numpy matrix of shape (m,n)\n",
        "\n",
        "    Returns:\n",
        "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    # Apply exp() element-wise to x to get x_exp.\n",
        "    x_exp =\n",
        "    # Create a vector x_sum that sums each row of x_exp.\n",
        "    x_sum =\n",
        "    # Compute softmax(x) by dividing results of 2 previous steps.\n",
        "    s =\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfbO0iDv1-b0"
      },
      "outputs": [],
      "source": [
        "x = np.array([\n",
        "    [9, 2, 5, 0, 0],\n",
        "    [7, 5, 0, 0 ,0]], dtype=float)\n",
        "print(\"softmax(x) = \" + str(softmax(x)))\n",
        "print(\"sum of each row of softmax(x) = \" + str(tf.reduce_sum(softmax(x), axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rl37MJU1-b1"
      },
      "source": [
        "**Note**:\n",
        "- If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). **x_exp/x_sum** works due to python broadcasting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp7p0xH01-b1"
      },
      "source": [
        "<font color='blue'>\n",
        "**What you need to remember:**\n",
        "\n",
        "- tf.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
        "- the sigmoid function and its gradient\n",
        "- image2vector is commonly used in deep learning\n",
        "- tf.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.\n",
        "- broadcasting is extremely useful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "W6S-DTw01-b1"
      },
      "source": [
        "<a name='1-2'></a>\n",
        "## 2 - Vectorization\n",
        "\n",
        "In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy2po4Aj1-b1"
      },
      "source": [
        "<a name='2-1'></a>\n",
        "### 2.1 Implement the L1 and L2 loss functions\n",
        "<a name='2-1-1'></a>\n",
        "#### 2.1.1 L1 loss:\n",
        "**Exercise**: Implement the vectorized version of the L1 loss. You may find the function tf.abs(x) (absolute value of x) useful.\n",
        "\n",
        "**Reminder**:\n",
        "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
        "- L1 loss is defined as:\n",
        "$$\\begin{align*} & L_1(\\hat{y}, y) = \\frac{1}{m}\\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlBEybMh1-b1"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: L1\n",
        "def L1(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "\n",
        "    Returns:\n",
        "    loss -- the value of the L1 loss function defined above\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdA8NxHm1-b1"
      },
      "outputs": [],
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L1 = \" + str(L1(yhat,y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPiI129D1-b1"
      },
      "source": [
        "<a name='2-1-2'></a>\n",
        "#### 2.1.2 L2 loss:\n",
        "**Exercise**: Implement the vectorized version of the L2 loss. There are several way of implementing the L2 loss.\n",
        "\n",
        "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\frac{1}{m}\\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01_0s1HN1-b1"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: L2\n",
        "\n",
        "def L2(yhat, y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    yhat -- vector of size m (predicted labels)\n",
        "    y -- vector of size m (true labels)\n",
        "\n",
        "    Returns:\n",
        "    loss -- the value of the L2 loss function defined above\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSD9Wyf91-b1"
      },
      "outputs": [],
      "source": [
        "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
        "y = np.array([1, 0, 0, 1, 1])\n",
        "print(\"L2 = \" + str(L2(yhat,y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-ALC1k1-b1"
      },
      "source": [
        "<font color='blue'>\n",
        "**What to remember:**\n",
        "\n",
        "- Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
        "- You have reviewed the L1 and L2 loss.\n",
        "- You are familiar with many tensorflow functions etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57knM8jrYZ2t"
      },
      "source": [
        "<a name='3'></a>\n",
        "# Part2: Intro to TensorFlow\n",
        "\n",
        "In this part of the assignment, you'll get exposure to using TensorFlow and learn how it can be used for solving deep learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QNMcdP4m3Vs"
      },
      "source": [
        "## 1.1 Why is TensorFlow called TensorFlow?\n",
        "\n",
        "TensorFlow is called 'TensorFlow' because it handles the flow (node/mathematical operation) of Tensors, which are data structures that you can think of as multi-dimensional arrays.\n",
        "The ```shape``` of a Tensor defines its number of dimensions and the size of each dimension. The ```rank``` of a Tensor provides the number of dimensions (n-dimensions) -- you can also think of this as the Tensor's order or degree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFeBBe1IouS3"
      },
      "outputs": [],
      "source": [
        "### Defining higher-order Tensors ###\n",
        "\n",
        "'''TODO: Define a 2-d Tensor'''\n",
        "matrix = # TODO\n",
        "\n",
        "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(matrix).numpy() == 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv1fTn_Ya_cz"
      },
      "outputs": [],
      "source": [
        "'''TODO: Define a 4-d Tensor.'''\n",
        "# Use tf.zeros to initialize a 4-d Tensor of zeros with size 10 x 256 x 256 x 3.\n",
        "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
        "images = # TODO\n",
        "\n",
        "assert isinstance(images, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
        "assert tf.rank(images).numpy() == 4, \"matrix must be of rank 4\"\n",
        "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD3VO-LZYZ2z"
      },
      "source": [
        "## 1.2 Computations on Tensors\n",
        "\n",
        "A convenient way to think about and visualize computations in TensorFlow is in terms of graphs. We can define this graph in terms of Tensors, which hold data, and the mathematical operations that act on these Tensors in some order. Let's look at a simple example, and define this computation using TensorFlow:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/add-graph.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_YJrZsxYZ2z"
      },
      "outputs": [],
      "source": [
        "# Create the nodes in the graph, and initialize values\n",
        "a = tf.constant(15)\n",
        "b = tf.constant(61)\n",
        "\n",
        "# Add them!\n",
        "c1 = tf.add(a,b)\n",
        "c2 = a + b # TensorFlow overrides the \"+\" operation so that it is able to act on Tensors\n",
        "print(c1)\n",
        "print(c2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbfv_QOiYZ23"
      },
      "source": [
        "Notice how we've created a computation graph consisting of TensorFlow operations, and how  the output is a Tensor with value 76 -- we've just created a computation graph consisting of operations, and it's executed them and given us back the result.\n",
        "\n",
        "Now let's consider a slightly more complicated example:\n",
        "\n",
        "<img src=\"https://i.ibb.co/VQvRN3y/computational-graph.png\" alt=\"computational-graph\" border=\"0\">\n",
        "\n",
        "Here, we take four inputs, `a, b, x, y`, and compute an output `f`. Each node in the graph represents an operation that takes some input, does some computation, and passes its output to another node.\n",
        "\n",
        "Let's define a simple function in TensorFlow to construct this computation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJnfzpWyYZ23",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "### Defining Tensor computations ###\n",
        "\n",
        "# Construct a simple computation function\n",
        "def func(x,a,b,y):\n",
        "  '''TODO: Define the operation for c, d, e, f (use tf.add, tf.subtract, tf.multiply).'''\n",
        "  c =\n",
        "  d =\n",
        "  e =\n",
        "  f =\n",
        "  return f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwrRfDMS2-oy"
      },
      "source": [
        "Now, we can call this function to execute the computation graph given some inputs `a,b`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnwsf8w2uF7p"
      },
      "outputs": [],
      "source": [
        "# Consider example values for a,b\n",
        "x, a, b, y = 1.5, 2.5, 2, 4\n",
        "# Execute the computation\n",
        "f_out = func(x,a,b,y)\n",
        "print(f_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HqgUIUhYZ29"
      },
      "source": [
        "Notice how our output is a Tensor with value defined by the output of the computation, and that the output has no shape as it is a single scalar value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Miv9p1T1-b5"
      },
      "source": [
        "## 1.3 Gradients Computations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`GradientTape` provides an extremely flexible framework for automatic differentiation. In order to back propagate errors through a neural network, we track forward passes on the Tape, use this information to determine the gradients, and then use these gradients for optimization using SGD."
      ],
      "metadata": {
        "id": "zER4WpxFLNe6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdkqk8pw5yJM"
      },
      "outputs": [],
      "source": [
        "### Gradient computation with GradientTape ###\n",
        "\n",
        "# y = x^2\n",
        "# Example: x = 3.0\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Initiate the gradient tape\n",
        "with tf.GradientTape() as tape:\n",
        "  # Define the function\n",
        "  y = x * x\n",
        "\n",
        "# Access the gradient -- derivative of y with respect to x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "\n",
        "assert dy_dx.numpy() == 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhU5metS5xF3"
      },
      "source": [
        "In training neural networks, we use differentiation and stochastic gradient descent (SGD) to optimize a loss function. Now that we have a sense of how `GradientTape` can be used to compute and access derivatives, we will look at an example where we use automatic differentiation and SGD to find the minimum of\n",
        "$$L=(wx-y_{true})^2$$\n",
        "Here $y_{true}$ is a variable for a desired value we are trying to optimize for; $L$ represents a loss that we are trying to  minimize. While we can clearly solve this problem analytically ($w_{min}=\\frac{y_{true}}{x}$), considering how we can compute this using `GradientTape` sets us up nicely for future assignments where we use gradient descent to optimize entire neural network losses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "attributes": {
          "classes": [
            "py"
          ],
          "id": ""
        },
        "id": "7g1yWiSXqEf-"
      },
      "outputs": [],
      "source": [
        "### Function minimization with automatic differentiation and SGD ###\n",
        "tf.random.set_seed(1234)\n",
        "# Initialize a random value for our initial w\n",
        "w = tf.Variable([tf.random.normal([1])])\n",
        "print(\"Initializing w={}\".format(w.numpy()))\n",
        "\n",
        "x = 2.0\n",
        "\n",
        "learning_rate = 3e-2 # learning rate for SGD\n",
        "history = []\n",
        "history.append(w.numpy()[0])\n",
        "# Define the target value\n",
        "y_true = 4.0\n",
        "\n",
        "# We will run SGD for a number of iterations. At each iteration, we compute the loss,\n",
        "#   compute the derivative of the loss with respect to x, and perform the SGD update.\n",
        "for i in range(500):\n",
        "    with tf.GradientTape() as tape:\n",
        "        '''TODO: define the loss as described above'''\n",
        "        # TODO\n",
        "\n",
        "  # loss minimization using gradient tape\n",
        "    grad =  # TODO: compute the derivative of the loss with respect to x\n",
        "    new_w =  # TODO: sgd update eqtn\n",
        "    # TODO: update the value of w\n",
        "\n",
        "    history.append(w.numpy()[0])\n",
        "\n",
        "# Plot the evolution of wx as we optimize it towards y_true!\n",
        "fig = plt.figure(figsize = (10,7))\n",
        "plt.plot(np.array(history)*x)\n",
        "plt.plot([0, 500],[y_true,y_true])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('w value')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the final value of w\n",
        "print(w)"
      ],
      "metadata": {
        "id": "18vzHF-EE00Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJE7lt5F1-b5"
      },
      "source": [
        "The following cell shows the evolution of the w value during gradien descent starting from initial w value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVDzgf9b1-b6"
      },
      "outputs": [],
      "source": [
        "w = np.linspace(-2, 6, 200)\n",
        "loss_f = (w*x-y_true)**2\n",
        "loss = (np.array(history)*x-y_true)**2\n",
        "fig = plt.figure(figsize = (10,7))\n",
        "plt.title(\"Evolution of the cost function during gradient descent\", fontsize=15)\n",
        "plt.plot(w,loss_f)\n",
        "plt.plot(history, loss,'*', label = \"Cost function\")\n",
        "plt.xlabel('Weight', fontsize=11)\n",
        "plt.ylabel('Loss', fontsize=11)\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbnwy_JD1-b6"
      },
      "source": [
        "#### Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "768rtHXM1-b6"
      },
      "source": [
        "**Exercise**: Try the previous code blocks with learning rates ${0.3, 0.000005}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeXduhAN1-b6"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "#### Implement SGD with learning_rate = 0.3\n",
        "tf.random.set_seed(1234)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw4dxFZS1-b6"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "#### Plot w value evolution against cost function\n",
        "#### -- make sure to remove the nans and inf from history before plotting\n",
        "#### -- constrain the value of w and history to be between -10 and 10 before plotting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGxd-3iT1-b6"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "#### Implement SGD with learning_rate = 0.000005\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUEGs1Er1-b6"
      },
      "outputs": [],
      "source": [
        "## TODO\n",
        "#### Plot w value evolution against cost function\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxcdEwu11-b6"
      },
      "source": [
        "# Part3: A neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO62anib1-b6"
      },
      "source": [
        "In the tutorial we learned how to create a network model that predicts the handwritten digits from the MNIST dataset. This time we are trying recognize different items of clothing, trained from a dataset containing 10 different types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_n1U5do3u_F"
      },
      "source": [
        "The Fashion MNIST data is available directly in the tf.keras datasets API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ffmmPN81-b7"
      },
      "source": [
        "### Question 1 Loading and Viewing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9giYpQD1-b7"
      },
      "source": [
        "The Fashion MNIST data is available directly in the tf.keras datasets API.\n",
        "- **Q** Load it like we did in the tutorial from keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmxkHFpt31bM"
      },
      "outputs": [],
      "source": [
        " #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9Cv9Bn11-b7"
      },
      "source": [
        "- **Q** Normalize it like we did in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRH19pWs6ZDn"
      },
      "outputs": [],
      "source": [
        " #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_dNFJQj1-b7"
      },
      "source": [
        "- **Q** Display 10 *random* images from the training images in 1 figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0172X_Zy1-b7"
      },
      "outputs": [],
      "source": [
        "random.seed(1234)\n",
        "#TODO#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dky4nQEb1-b7"
      },
      "source": [
        "### Question 2 The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIn7S9gf62ie"
      },
      "source": [
        "Let's now design the model. Design a sequential neural network model with 2 hidden dense layers and 1 output dense layer. Use ReLu as activation function for the hidden layers and softmax as the activation for the output layer.\n",
        "\n",
        "For the hidden layers' neurons, their number is left to your decision 😏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mAyndG3kVlK"
      },
      "outputs": [],
      "source": [
        "#TODO# Sequential Model\n",
        "\n",
        "model ="
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the some model but using the `Functional` API"
      ],
      "metadata": {
        "id": "z9NCNycfYK9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO# Functional Model\n",
        "\n",
        "model ="
      ],
      "metadata": {
        "id": "N1bENiMkYSIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model with the appropriate crossentropy loss (take note of type of label) and Adam optimizer. Use accuracy for the metrics."
      ],
      "metadata": {
        "id": "x4aQxE_vMcmD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLMdl9aP8nQ0"
      },
      "outputs": [],
      "source": [
        "#TODO# Compile the model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(training_images, training_labels, epochs=5)"
      ],
      "metadata": {
        "id": "28qjqoZcM197"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "id": "BsNS77d1TpFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rquQqIx4AaGR"
      },
      "source": [
        "Run the below code: It creates a set of classifications for each of the test images, and then prints the first entry in the classifications. The output, after you run it is a list of numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyEIki0z_hAD"
      },
      "outputs": [],
      "source": [
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdzqbQhRArzm"
      },
      "source": [
        "Hint: try running print(test_labels[0]) -- and you'll get a 9. Does that help you understand why this list looks the way it does?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnBGOrMiA1n5"
      },
      "outputs": [],
      "source": [
        "print(test_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUs7eqr7uSvs",
        "tags": []
      },
      "source": [
        "- **Q** What does this list represent?\n",
        "\n",
        "\n",
        "1.   It's 10 random meaningless values\n",
        "2.   It's the first 10 classifications that the computer made\n",
        "3.   It's the probability that this item is each of the 10 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAbr92RTA67u"
      },
      "source": [
        "- TODO (Answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD4kC6TBu-69"
      },
      "source": [
        "**Q** How do you know that this list tells you that the item is an ankle boot?\n",
        "\n",
        "\n",
        "1.   There's not enough information to answer that question\n",
        "2.   The 10th element on the list is the biggest, and the ankle boot is labelled 9\n",
        "2.   The ankle boot is label 9, and there are 0->9 elements in the list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEpYX8zE1-b8"
      },
      "source": [
        "- TODO (Answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgQSIfDSOWv6"
      },
      "source": [
        "Let's now look at the layers in your model. Experiment with different values for the dense layer. What different results do you get for loss, training time etc? Why do you think that's the case?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOOEnHZFv5cS"
      },
      "source": [
        "**Q** Use a larger number of neurons in the 2 hidden layers -- What's the impact?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBqallpf1-b8"
      },
      "source": [
        "- TODO (Answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSZSwV5UObQP"
      },
      "outputs": [],
      "source": [
        "#TODO# create model\n",
        "model =\n",
        "\n",
        "#TODO# Compile the model\n",
        "\n",
        "\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS3vVkOgCDGZ"
      },
      "source": [
        "**Q** Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1. What would be the impact of removing that? Here's the complete code to give it a try. Why do you think you get different results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeapTMDK1-b9"
      },
      "source": [
        "- TODO (Answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KYiynj41-b9"
      },
      "outputs": [],
      "source": [
        "#### CODE FOR DATA NOT NORMALIZED\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDqNAqrpCNg0"
      },
      "outputs": [],
      "source": [
        "#### CODE FOR DATA NORMALIZED\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.fit(training_images, training_labels, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eI6DUic-6jo"
      },
      "source": [
        "References:\n",
        "- MIT 6.S191\n",
        "- DL.ai\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WBk0ZDWY-ff8"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}