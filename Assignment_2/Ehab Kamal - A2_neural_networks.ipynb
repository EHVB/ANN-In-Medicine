{"cells":[{"cell_type":"markdown","id":"d2fed8c8-3b58-464a-b89d-df85d934f740","metadata":{"id":"d2fed8c8-3b58-464a-b89d-df85d934f740"},"source":["---\n","# Cairo University Faculty of Engineering\n","## Deep Learning\n","## Assignment 2\n","\n","---"]},{"cell_type":"markdown","id":"9f09594d-0c42-4205-a0af-77097e41f555","metadata":{"id":"9f09594d-0c42-4205-a0af-77097e41f555"},"source":["Please write your full name here\n","- **Name** : \"-----------\""]},{"cell_type":"code","execution_count":null,"id":"02dc8d6e-7d9a-48c0-a84d-fdd3c63c9c78","metadata":{"id":"02dc8d6e-7d9a-48c0-a84d-fdd3c63c9c78"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import tensorflow as tf\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import pandas as pd\n","import math\n","\n","from sklearn.datasets import make_blobs  #To generate artificial data\n","from keras.optimizers import SGD\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","id":"caa1ef2a-e2e4-45b6-9f95-ad5a9d8e7391","metadata":{"id":"caa1ef2a-e2e4-45b6-9f95-ad5a9d8e7391"},"source":["**Use TensorFlow unless mentioned otherwise**"]},{"cell_type":"markdown","id":"4f383a40-2b6d-4bb3-bb45-4b6602896480","metadata":{"tags":[],"id":"4f383a40-2b6d-4bb3-bb45-4b6602896480"},"source":["# Part 2: Regression"]},{"cell_type":"markdown","id":"0ddb4513-a883-475c-acb4-7daf8a1434a7","metadata":{"jp-MarkdownHeadingCollapsed":true,"origin_pos":0,"tags":[],"id":"0ddb4513-a883-475c-acb4-7daf8a1434a7"},"source":["In this part, (**we will implement the entire linear regression method from scratch,\n","including the data pipeline, the model,\n","the loss function, and the minibatch stochastic gradient descent optimizer.**)\n","You will rely only on tensors and auto differentiation.\n"]},{"cell_type":"markdown","id":"9885bf7b-0575-456d-a16c-997636bb6b3b","metadata":{"id":"9885bf7b-0575-456d-a16c-997636bb6b3b"},"source":["we will use $n$ to denote\n","the number of examples in our dataset.\n","We index the data examples by $i$, denoting each input\n","as $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$\n","and the corresponding label as $y^{(i)}$.\n","\n","\n","**Linear Model**\n","\n","\n","We will often find it convenient\n","to refer to features of our entire dataset of $n$ examples\n","via the *design matrix* $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$.\n","Here, $\\mathbf{X}$ contains one row for every example\n","and one column for every feature.\n","\n","For a collection of features $\\mathbf{X}$,\n","the predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$\n","can be expressed via the matrix-vector product:\n","\n","$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b,$$\n","\n","\n","**Loss Function**\n","\n","The most popular loss function in regression problems\n","is the squared error.\n","When our prediction for an example $i$ is $\\hat{y}^{(i)}$\n","and the corresponding true label is $y^{(i)}$,\n","the squared error is given by:\n","\n","$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n","\n","\n","A regression problem for a one-dimensional case is shown below:\n","\n","<img src=\"https://i.ibb.co/8cXVC31/fit-linreg.png\" alt=\"fit-linreg\" border=\"0\">\n","\n","To measure the quality of a model on the entire dataset of $n$ examples,\n","we simply average (or equivalently, sum)\n","the losses on the training set.\n","\n","$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n","\n","When training the model, we want to find parameters ($\\mathbf{w}^*, b^*$)\n","that minimize the total loss across all training examples:\n","\n","$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$\n"]},{"cell_type":"markdown","id":"eddedadc-6641-4c7a-8134-79549b655a62","metadata":{"id":"eddedadc-6641-4c7a-8134-79549b655a62"},"source":["**Minibatch Stochastic Gradient Descent**\n","\n","\n","The key technique for optimizing models\n","consists of iteratively reducing the error\n","by updating the parameters in the direction\n","that incrementally lowers the loss function --> *gradient descent*.\n","\n","We will settle for sampling a random minibatch of examples\n","every time we need to compute the update --> *minibatch stochastic gradient descent*.\n","\n","We can express the update mathematically as follows\n","($\\partial$ denotes the partial derivative):\n","\n","$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n"]},{"cell_type":"markdown","id":"b5b67706-78b0-428a-b984-03e8a44f2daf","metadata":{"id":"b5b67706-78b0-428a-b984-03e8a44f2daf"},"source":["## Synthetic Data, Simple Model\n","### Generating the Dataset\n","\n","To keep things simple, we will [**construct an artificial dataset\n","according to a linear model with additive noise.**]\n","\n","In the following code snippet, we generate a dataset\n","containing 1000 examples, each consisting of 2 features\n","sampled from a standard normal distribution.\n","Thus our synthetic dataset will be a matrix\n","$\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n","\n","(**The true parameters generating our dataset will be\n","$\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$,\n","and**) our synthetic labels will be assigned according\n","to the following linear model with the noise term $\\epsilon$:\n","\n","(**$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$**)"]},{"cell_type":"code","execution_count":null,"id":"6c56cf52-1cbe-4f80-9551-9988b28fe967","metadata":{"origin_pos":6,"tab":["tensorflow"],"id":"6c56cf52-1cbe-4f80-9551-9988b28fe967"},"outputs":[],"source":["def synthetic_data(w, b, num_examples):\n","    \"\"\"Generate y = Xw + b + noise.\"\"\"\n","    X = tf.zeros((num_examples, w.shape[0]))\n","    X += tf.random.normal(shape=X.shape)\n","    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b\n","    y += tf.random.normal(shape=y.shape, stddev=0.01)\n","    y = tf.reshape(y, (-1, 1))\n","    return X, y"]},{"cell_type":"code","execution_count":null,"id":"6f7fc2ce-cdd3-45f0-b632-f5b7fbd38155","metadata":{"origin_pos":7,"tab":["tensorflow"],"id":"6f7fc2ce-cdd3-45f0-b632-f5b7fbd38155"},"outputs":[],"source":["tf.random.set_seed(5)\n","true_w = tf.constant([2, -3.4])\n","true_b = 4.2\n","features, labels = synthetic_data(true_w, true_b, 1000)"]},{"cell_type":"code","execution_count":null,"id":"a088e27d-495b-461f-b483-5eaaa97d1dc8","metadata":{"origin_pos":9,"tab":["tensorflow"],"id":"a088e27d-495b-461f-b483-5eaaa97d1dc8"},"outputs":[],"source":["print('features:', features[0],'\\nlabel:', labels[0])"]},{"cell_type":"markdown","id":"af9a27cb-fcac-4856-99cb-724fa649c9a9","metadata":{"origin_pos":10,"id":"af9a27cb-fcac-4856-99cb-724fa649c9a9"},"source":["By generating a scatter plot using the second feature `features[:, 1]` and `labels`,\n","we can clearly observe the linear correlation between the two.\n"]},{"cell_type":"code","execution_count":null,"id":"8cbcb65e-1376-4879-8a6c-5dc2dbc4c945","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"8cbcb65e-1376-4879-8a6c-5dc2dbc4c945"},"outputs":[],"source":["# The semicolon is for displaying the plot only\n","plt.scatter(features[:, (1)].numpy(), labels.numpy(), 1);"]},{"cell_type":"markdown","id":"36253bc0-ba53-410c-86bb-7a2eb11f9592","metadata":{"origin_pos":5,"tags":[],"id":"36253bc0-ba53-410c-86bb-7a2eb11f9592"},"source":["### Reading the Dataset\n","\n","In the following code [**call upon the existing API in a framework to read data.**]\n","We pass in `features` and `labels` as arguments and specify `batch_size`\n","when instantiating a data iterator object.\n","Besides, the boolean value `is_train`\n","indicates whether or not\n","we want the data iterator object to shuffle the data\n","on each epoch (pass through the dataset).\n"]},{"cell_type":"markdown","id":"c28419fa-707f-4a30-8d28-f18ab5d73c31","metadata":{"id":"c28419fa-707f-4a30-8d28-f18ab5d73c31"},"source":["1. **Use the tf function from_tensor_slices to generate a tf dataset object with batch_size as input**\n","2. **Use is_train flag to determine whether to shuffle the dataset or not, set the buffer size appropriately**"]},{"cell_type":"code","execution_count":null,"id":"53aaab37-4b35-4ff3-9d72-292d260838a9","metadata":{"origin_pos":8,"tab":["tensorflow"],"id":"53aaab37-4b35-4ff3-9d72-292d260838a9"},"outputs":[],"source":["def load_array(data_arrays, batch_size, is_train=True):\n","    \"\"\"Construct a TensorFlow data iterator.\"\"\"\n","    #### YOUR CODE HERE ###\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"id":"1421cf52-54d8-4341-816c-a5d095bfb8c3","metadata":{"origin_pos":9,"tab":["tensorflow"],"id":"1421cf52-54d8-4341-816c-a5d095bfb8c3"},"outputs":[],"source":["batch_size = 10\n","data_iter = load_array((features, labels), batch_size)"]},{"cell_type":"markdown","id":"d6e0c589-effb-40ba-8149-3309692fed48","metadata":{"origin_pos":10,"id":"d6e0c589-effb-40ba-8149-3309692fed48"},"source":["3. **Use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator.**\n"]},{"cell_type":"code","execution_count":null,"id":"2144c8a5-c59c-4886-af3b-8f4353e8f7c6","metadata":{"origin_pos":11,"tab":["tensorflow"],"id":"2144c8a5-c59c-4886-af3b-8f4353e8f7c6"},"outputs":[],"source":["### YOUR CODE HERE ###\n"]},{"cell_type":"markdown","id":"24da4a06-3978-49c2-bcf4-cb4c1ae64f49","metadata":{"origin_pos":15,"id":"24da4a06-3978-49c2-bcf4-cb4c1ae64f49"},"source":["4. **Explain what the output shape in the prvious tensors means:**\n"]},{"cell_type":"markdown","id":"c7eda004-0643-4132-b737-c1f2fc2049d9","metadata":{"origin_pos":16,"tab":["tensorflow"],"id":"c7eda004-0643-4132-b737-c1f2fc2049d9"},"source":["**Answer:**\n"]},{"cell_type":"markdown","id":"5c83f595-3746-435e-a824-9b4011a5aa40","metadata":{"id":"5c83f595-3746-435e-a824-9b4011a5aa40"},"source":["5. **How many batches are in the **ENTIRE** data_iter ?**"]},{"cell_type":"markdown","id":"88eb5391-71a5-45ef-8af3-8f6a33b7793c","metadata":{"origin_pos":16,"tab":["tensorflow"],"id":"88eb5391-71a5-45ef-8af3-8f6a33b7793c"},"source":["**Answer:**"]},{"cell_type":"markdown","id":"71628cba-f449-4139-a02a-f4b9e6b53545","metadata":{"origin_pos":17,"tags":[],"id":"71628cba-f449-4139-a02a-f4b9e6b53545"},"source":["### Initializing Model Parameters\n","\n","6. **Initialize weights by sampling random numbers from a normal distribution with mean 0 and a standard deviation of 0.02, and setting the bias to 0.**\n","\n","Note: For the shapes of the weights and bias, look at the generating a dataset part\n"]},{"cell_type":"code","execution_count":null,"id":"db96e147-7256-4a1e-8c8a-6b89df19f117","metadata":{"origin_pos":20,"tab":["tensorflow"],"id":"db96e147-7256-4a1e-8c8a-6b89df19f117"},"outputs":[],"source":["## YOUR CODE HERE ##\n"]},{"cell_type":"markdown","id":"09bbcd34-e487-4751-b8ff-a0f932350562","metadata":{"origin_pos":21,"id":"09bbcd34-e487-4751-b8ff-a0f932350562"},"source":["### Defining the Model\n","\n","7. [**define our model, relating its inputs and parameters to its outputs.**]"]},{"cell_type":"code","execution_count":null,"id":"a892f99c-9846-4b4c-bf89-dd552ca2fd26","metadata":{"origin_pos":22,"tab":["tensorflow"],"id":"a892f99c-9846-4b4c-bf89-dd552ca2fd26"},"outputs":[],"source":["def linreg(X, w, b):\n","    \"\"\"\n","    The linear regression model.\n","    Input:\n","    - X: input matrix\n","    - w: weights\n","    - bias\n","    \"\"\"\n","    ## YOUR CODE HERE ##\n","\n","    return y_hat"]},{"cell_type":"markdown","id":"78acdc39-21c6-4125-b0de-b7a14a9c93eb","metadata":{"origin_pos":23,"id":"78acdc39-21c6-4125-b0de-b7a14a9c93eb"},"source":["### Defining the Loss Function\n","\n","8. (**define the loss function**): the squared loss function\n","as described in Loss Function definition above.\n","\n","Note: In the implementation, you need to transform the true value `y`\n","into the predicted value's shape `y_hat`.\n","The result returned by the following function\n","will also have the same shape as `y_hat`.\n"]},{"cell_type":"code","execution_count":null,"id":"37e04bc2-0d67-4204-b0bf-b99ff8a1d0d3","metadata":{"origin_pos":24,"tab":["tensorflow"],"id":"37e04bc2-0d67-4204-b0bf-b99ff8a1d0d3"},"outputs":[],"source":["def squared_loss(y_hat, y):\n","    \"\"\"Squared loss.\"\"\"\n","    ## YOUR CODE HERE ##\n","\n","    return loss"]},{"cell_type":"markdown","id":"212f6a87-1c81-4aa1-aebe-41334d719398","metadata":{"origin_pos":25,"id":"212f6a87-1c81-4aa1-aebe-41334d719398"},"source":["### Defining the Optimization Algorithm\n","\n","At each step, using one minibatch randomly drawn from our dataset,\n","we will estimate the gradient of the loss with respect to our parameters.\n","Next, we will update our parameters\n","in the direction that may reduce the loss.\n","\n","9. **Filll in the missing function below to apply the minibatch stochastic gradient descent update, given a set of parameters, a learning rate, and a batch size.**\n","\n","This function should return nothing\n","\n","Note: use assign_sub\n"]},{"cell_type":"code","execution_count":null,"id":"ddfe9ccd-271e-4fc3-ab9a-3720d6bf4b23","metadata":{"origin_pos":28,"tab":["tensorflow"],"id":"ddfe9ccd-271e-4fc3-ab9a-3720d6bf4b23"},"outputs":[],"source":["def sgd(params, grads, lr, batch_size):\n","    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n","\n","    ## YOUR CODE HERE ##\n"]},{"cell_type":"markdown","id":"9016fa58-4d4b-44b1-91a1-6f1e8995c9f7","metadata":{"origin_pos":29,"id":"9016fa58-4d4b-44b1-91a1-6f1e8995c9f7"},"source":["### Training\n","\n","10. **Implement the following loop**\n","\n","* For each epoch :\n","    * For each batch in data_iter:\n","        * Compute the loss\n","        * Compute gradient\n","        * Update parameters\n","    * Calculate and print the training loss at the end of each epoch\n","\n"]},{"cell_type":"markdown","source":["Loss = $ l({x}, {y}) $\n","\n","Gradient => $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n","\n","Parameters update => $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$"],"metadata":{"id":"FV4mqhvL6vQG"},"id":"FV4mqhvL6vQG"},{"cell_type":"markdown","source":["In each *epoch*,\n","we will iterate through the entire dataset\n","(using the `data_iter` function) once\n","passing through every example in the training dataset\n",".\n","\n","Don't forget to use GradientTape\n","\n","Set the number of epochs `num_epochs` and the learning rate `lr` to 5 and 0.02, respectively."],"metadata":{"id":"NctDJlGe7Odq"},"id":"NctDJlGe7Odq"},{"cell_type":"code","execution_count":null,"id":"50a93813-59c4-48e7-beaf-d27fb707a96f","metadata":{"origin_pos":30,"tab":["tensorflow"],"id":"50a93813-59c4-48e7-beaf-d27fb707a96f"},"outputs":[],"source":["lr = ####\n","num_epochs = #####\n","net = linreg\n","loss = squared_loss"]},{"cell_type":"code","execution_count":null,"id":"7f2d2d6e-1661-4737-b135-887221607b48","metadata":{"origin_pos":33,"tab":["tensorflow"],"id":"7f2d2d6e-1661-4737-b135-887221607b48"},"outputs":[],"source":["## YOUR CODE HERE ## The loop code\n"]},{"cell_type":"markdown","id":"393f4953-35dc-46c1-b495-96172232bc3f","metadata":{"origin_pos":34,"id":"393f4953-35dc-46c1-b495-96172232bc3f"},"source":["In this case, because we synthesized the dataset ourselves,\n","we know precisely what the true parameters are.\n","\n","11. [**evaluate our success in training by comparing the true parameters with those that we learned through our training loop**] (final w, b). They should turn out to be very close to each other.\n","\n","calculate the difference!!\n"]},{"cell_type":"code","execution_count":null,"id":"c99daff3-745a-43fd-898e-bbf1e3804c78","metadata":{"origin_pos":35,"tab":["tensorflow"],"id":"c99daff3-745a-43fd-898e-bbf1e3804c78"},"outputs":[],"source":["## YOUR CODE HERE ##\n","print(f'error in estimating w: #######')\n","print(f'error in estimating b: #######')"]},{"cell_type":"markdown","id":"3f7b518a-c2ac-4f0e-941b-98d300ed9963","metadata":{"id":"3f7b518a-c2ac-4f0e-941b-98d300ed9963"},"source":["## Diabetes Dataset"]},{"cell_type":"markdown","id":"06b07d9c-b52e-4f52-8e70-f7a4bebd63bb","metadata":{"id":"06b07d9c-b52e-4f52-8e70-f7a4bebd63bb"},"source":["In this section, we will use SKLEARN's Diabetes dataset"]},{"cell_type":"markdown","id":"e0cef42e-4494-4f16-b2d2-7cfe68c51e3d","metadata":{"id":"e0cef42e-4494-4f16-b2d2-7cfe68c51e3d"},"source":["### Loading the Dataset"]},{"cell_type":"markdown","id":"2aea7770-afb6-45f2-93b1-b9c72df36bc0","metadata":{"id":"2aea7770-afb6-45f2-93b1-b9c72df36bc0"},"source":["1. **Load the Diabetes dataset from sklearn**"]},{"cell_type":"code","execution_count":null,"id":"f07dc3b6-c48e-4a37-8f73-3500f344e717","metadata":{"id":"f07dc3b6-c48e-4a37-8f73-3500f344e717"},"outputs":[],"source":["from sklearn.datasets import load_diabetes\n","## YOUR CODE HERE ##\n","# Load the diabetes dataset\n","\n"]},{"cell_type":"markdown","id":"aa95939c-f94a-4383-9e4b-18c86af1cebf","metadata":{"id":"aa95939c-f94a-4383-9e4b-18c86af1cebf"},"source":["2. **Look at the keys of diabetes_dataset dictionary**"]},{"cell_type":"code","execution_count":null,"id":"1c92cb43-cb3d-47f1-8535-c4439f585412","metadata":{"id":"1c92cb43-cb3d-47f1-8535-c4439f585412"},"outputs":[],"source":["## YOUR CODE HERE ##\n","\n","### Look at keys to determine the data\n","\n"]},{"cell_type":"markdown","id":"484a2460-7023-4468-9399-3a40d00ba55c","metadata":{"id":"484a2460-7023-4468-9399-3a40d00ba55c"},"source":["3. **Use the key DESCR to understand the dataset**"]},{"cell_type":"code","execution_count":null,"id":"87437737-e0d0-4e18-a841-ab214ed2261b","metadata":{"id":"87437737-e0d0-4e18-a841-ab214ed2261b"},"outputs":[],"source":["## YOUR CODE HERE ##\n"]},{"cell_type":"markdown","id":"6472180c-ebb8-44f9-8bab-c507e256ef39","metadata":{"id":"6472180c-ebb8-44f9-8bab-c507e256ef39"},"source":["4. **Save the data and target variables in numpy arrays and print their shapes**"]},{"cell_type":"code","execution_count":null,"id":"8fc2a160-b33f-44c5-83a7-08beb821a64a","metadata":{"id":"8fc2a160-b33f-44c5-83a7-08beb821a64a"},"outputs":[],"source":["### START CODE HERE ### (≈ 2 lines of code)\n","data =\n","targets =\n","### END CODE HERE ###\n","\n","print ('The shape of data is: ' + str(data.shape))\n","print ('The shape of targets is: ' + str(targets.shape))\n","print ('I have f = %d features!' % (data.shape[1]))\n","print ('I have m = %d examples!' % (data.shape[0]))"]},{"cell_type":"markdown","id":"52f4ad51-a6a3-4cfc-9c64-6e7a44545d66","metadata":{"id":"52f4ad51-a6a3-4cfc-9c64-6e7a44545d66"},"source":["5. **What are the ranges of each column in features and the target column?**\n","    - *Hint* you might find it helpful to convert to pandas dataframe and use \".describe\""]},{"cell_type":"code","execution_count":null,"id":"1e4d03cc-c705-4e4e-81f7-6d49a6e86fbd","metadata":{"id":"1e4d03cc-c705-4e4e-81f7-6d49a6e86fbd"},"outputs":[],"source":["### START CODE HERE ###\n","\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","id":"742f762c-fe10-42be-a3d6-7cbb4df68c0d","metadata":{"id":"742f762c-fe10-42be-a3d6-7cbb4df68c0d"},"source":["### Preparing the data"]},{"cell_type":"markdown","id":"4c879c44-aec4-4c29-aa2a-76a1d48171e6","metadata":{"id":"4c879c44-aec4-4c29-aa2a-76a1d48171e6"},"source":["6. **Split the data into train and test set using sklearn train_test_split.** Have the test set as 10% of data"]},{"cell_type":"code","execution_count":null,"id":"ced08578-8d2c-4e2b-ae34-4f6fa8f4326d","metadata":{"id":"ced08578-8d2c-4e2b-ae34-4f6fa8f4326d"},"outputs":[],"source":["np.random.seed(0)\n","### START CODE HERE ###\n","# Split the data into train and test sets\n","\n","\n","### END CODE HERE ###\n","\n","print(\"Shape of training data is\", train_data.shape)\n","print(\"Shape of training targets is\", train_targets.shape)\n","print(\"Shape of test data is\", test_data.shape)\n","print(\"Shape of test targets is\", test_targets.shape)"]},{"cell_type":"markdown","id":"e18cbdeb-252c-450b-b85a-d35d5989f928","metadata":{"id":"e18cbdeb-252c-450b-b85a-d35d5989f928"},"source":["- *feature-wise normalization*: for each feature in the input data (a column in the input data matrix), we subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in NumPy.\n","- **Question:** Should we normalize features? WHY?\n","    - ANSWER:"]},{"cell_type":"markdown","id":"e679d2b1-eded-4768-8954-c6cdfde3e207","metadata":{"id":"e679d2b1-eded-4768-8954-c6cdfde3e207"},"source":["7. **normalize the targets**"]},{"cell_type":"code","execution_count":null,"id":"f18246d4-dbd8-403a-9db9-a72360eaaebb","metadata":{"id":"f18246d4-dbd8-403a-9db9-a72360eaaebb"},"outputs":[],"source":["### START CODE HERE ### (≈ 6 lines of code)\n","\n","\n","\n","### END CODE HERE ###"]},{"cell_type":"markdown","id":"130ed50b-010c-488f-b1d0-aa2919fdf01d","metadata":{"id":"130ed50b-010c-488f-b1d0-aa2919fdf01d"},"source":["### Building your model"]},{"cell_type":"markdown","id":"9dca53c6-d8c6-4a93-876e-080cb537e98a","metadata":{"id":"9dca53c6-d8c6-4a93-876e-080cb537e98a"},"source":["7. **Use tf.keras.Sequential to build a model with:**\n","    - 6 hidden layers each having 128 neurons and relu activation.\n","    - 1 output layer\n","    - use input_shape argument to specify input size in 1st layer\n","- **Question:** How many neurons should be in output layer? What should be the activation?\n","    - ANSWER:"]},{"cell_type":"code","execution_count":null,"id":"cc9bc68e-d3a4-42c6-b1e1-42c2c97fa651","metadata":{"id":"cc9bc68e-d3a4-42c6-b1e1-42c2c97fa651"},"outputs":[],"source":["def build_model(input_shape):\n","    ### START CODE HERE ### ()\n","\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"cfe03837-d15a-4879-b9db-0477d01baa23","metadata":{"id":"cfe03837-d15a-4879-b9db-0477d01baa23"},"outputs":[],"source":["# Print the model summary\n","input_shape=(########)\n","model = build_model(input_shape)\n","print(model.summary())"]},{"cell_type":"markdown","id":"114f2e34-fa34-4312-bbe8-c929d350c89f","metadata":{"id":"114f2e34-fa34-4312-bbe8-c929d350c89f"},"source":["8. **Compile the model using optimizer=Adam, loss=mean squared loss, metrics=mean absoluute error:**"]},{"cell_type":"code","execution_count":null,"id":"8a438445-aac4-4913-8012-6d7095b09ed1","metadata":{"id":"8a438445-aac4-4913-8012-6d7095b09ed1"},"outputs":[],"source":["### START CODE HERE ### ()\n"]},{"cell_type":"markdown","id":"f9eb30a2-e09f-4298-8210-998eb79eb7a9","metadata":{"id":"f9eb30a2-e09f-4298-8210-998eb79eb7a9"},"source":["Note that we compile the model with the mse loss function—mean squared error, the\n","square of the difference between the predictions and the targets. We’re also monitoring a new metric during training: mean absolute error (MAE). It’s the\n","absolute value of the difference between the predictions and the targets."]},{"cell_type":"markdown","id":"36507d6f-cd6d-47fa-b74c-fd7e14c9d9ba","metadata":{"id":"36507d6f-cd6d-47fa-b74c-fd7e14c9d9ba"},"source":["9. **FIT the model using for 110 epochs, 64 batch size and a validation split of 0.1**"]},{"cell_type":"code","execution_count":null,"id":"1f63ab80-1353-4997-aba5-4e6299bee5d7","metadata":{"tags":[],"id":"1f63ab80-1353-4997-aba5-4e6299bee5d7"},"outputs":[],"source":["### START CODE HERE ### ()\n"]},{"cell_type":"code","execution_count":null,"id":"e2000d08-a40d-4bc8-ac8c-87c30111c670","metadata":{"id":"e2000d08-a40d-4bc8-ac8c-87c30111c670"},"outputs":[],"source":["# Evaluate the model on the test set\n"]},{"cell_type":"code","execution_count":null,"id":"3a0de6a8-b049-4221-972e-689b166c71aa","metadata":{"id":"3a0de6a8-b049-4221-972e-689b166c71aa"},"outputs":[],"source":["# Plot the training and validation loss\n","\n","plt.plot(##########)\n","plt.plot(############)\n","plt.title('Loss vs. epochs')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"5fea592c-3cd2-4fb6-b32b-d9da751763fd","metadata":{"id":"5fea592c-3cd2-4fb6-b32b-d9da751763fd"},"source":["9. **Diagnose the learning curve:**\n","    - Why is the validation loss much higher than training loss?\n","    - What's this phenomenon called?\n","    - Mention 3 methods to reduce this."]},{"cell_type":"markdown","id":"b8fd101e-f9d1-49bc-9101-1157932be789","metadata":{"id":"b8fd101e-f9d1-49bc-9101-1157932be789"},"source":["- **answer**:\n","    - ___\n","    - ____\n","    - ___"]},{"cell_type":"markdown","id":"f186ab89-4e72-4379-b458-843fbba8f1ef","metadata":{"id":"f186ab89-4e72-4379-b458-843fbba8f1ef"},"source":["10. **Use one of the methods you mentioned and retrain the model then plot the learning curves**"]},{"cell_type":"code","execution_count":null,"id":"aa69dd9f-6304-4115-a547-8f02caa613bf","metadata":{"id":"aa69dd9f-6304-4115-a547-8f02caa613bf"},"outputs":[],"source":["#### YOUR CODE HERE\n"]},{"cell_type":"markdown","id":"39417149-9f72-4fde-945e-7ea7a649b01f","metadata":{"id":"39417149-9f72-4fde-945e-7ea7a649b01f"},"source":["# Part 3 Batches and Epochs"]},{"cell_type":"markdown","id":"6b167f30-1712-4875-8643-985d6bff5894","metadata":{"id":"6b167f30-1712-4875-8643-985d6bff5894"},"source":["In this part of the assignment we will create a synthetic data to play with.\n","The data will have 2 features and 3 target classes --> multiclass classification problem"]},{"cell_type":"code","execution_count":null,"id":"5478a898-76b8-4764-9194-54920d6889f8","metadata":{"id":"5478a898-76b8-4764-9194-54920d6889f8"},"outputs":[],"source":["# prepare train and test dataset\n","def prepare_data():\n","    #  generate classification dataset with 3 centers (labels/classes)\n","    X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n","\n","    # plot data\n","    for class_value in range(3):\n","        # select indices of points with the class label\n","        row_ix = np.where(y == class_value)\n","        # scatter plot for points with a different color\n","        plt.scatter(X[row_ix, 0], X[row_ix, 1])\n","    # show plot\n","    plt.show()\n","\n","    # one hot encode output variable to convert from integers to binary class\n","    y = to_categorical(y)\n","\n","    # split into train and test\n","    n_train = 500\n","    X_train, X_test = X[:n_train, :], X[n_train:, :]\n","    y_train, y_test = y[:n_train], y[n_train:]\n","    return X_train, y_train, X_test, y_test\n"]},{"cell_type":"code","execution_count":null,"id":"32e2cf07-c958-46c3-a453-f1bead3502b0","metadata":{"id":"32e2cf07-c958-46c3-a453-f1bead3502b0"},"outputs":[],"source":["# prepare dataset\n","X_train, y_train, X_test, y_test = prepare_data()\n"]},{"cell_type":"markdown","id":"780221cc-5e13-4fd9-a940-38d43232c17b","metadata":{"id":"780221cc-5e13-4fd9-a940-38d43232c17b"},"source":["- **Create a model with:**\n","    -  1 hidden dense layer (50 neurons), activation relu, , kernel_initializer he_uniform\n","    - 1 output layer\n","    - compile the model with SGD (learning rate 0.01 and momentum 0.9) optimizer and categorical crossentropy and accuracy inside build function"]},{"cell_type":"code","execution_count":null,"id":"72840561-d883-4b04-85c0-aa3c25d88767","metadata":{"id":"72840561-d883-4b04-85c0-aa3c25d88767"},"outputs":[],"source":["def build_model():\n","\n","    ##### YOUR CODE HERE\n","\n","\n","\n","\n","    return model"]},{"cell_type":"markdown","id":"9f77f064-219f-4130-95d6-6f55c500d8aa","metadata":{"id":"9f77f064-219f-4130-95d6-6f55c500d8aa"},"source":["- Create a function to fit and build the model with different batch sizes"]},{"cell_type":"markdown","id":"36e5cb37-cf36-4f8f-9d2f-2adbeeea41bb","metadata":{"id":"36e5cb37-cf36-4f8f-9d2f-2adbeeea41bb"},"source":["Use epochs = 125"]},{"cell_type":"code","execution_count":null,"id":"8b6d1185-925a-4a4e-9815-29a3ca1a62cf","metadata":{"id":"8b6d1185-925a-4a4e-9815-29a3ca1a62cf"},"outputs":[],"source":["# fit a model and plot learning curve\n","def fit_model(X_train, y_train, X_test, y_test, n_batch):\n","    ##### YOUR CODE HERE\n","\n","\n","    return history"]},{"cell_type":"markdown","id":"b2539b4f-8829-4df1-9a45-a9c438a37baf","metadata":{"id":"b2539b4f-8829-4df1-9a45-a9c438a37baf"},"source":["### Hyperparameter Tuning"]},{"cell_type":"markdown","id":"fb875222-fd78-4ed5-a33f-a9c085bd5534","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"fb875222-fd78-4ed5-a33f-a9c085bd5534"},"source":["#### Batch size"]},{"cell_type":"markdown","id":"3c2c8424-0b59-45be-8bc3-0f310fb8ed30","metadata":{"id":"3c2c8424-0b59-45be-8bc3-0f310fb8ed30"},"source":["##### Size =  1"]},{"cell_type":"markdown","id":"77369330-70e7-41eb-a8ff-40eae827ea7d","metadata":{"id":"77369330-70e7-41eb-a8ff-40eae827ea7d"},"source":["10. **train for 1 batch size**"]},{"cell_type":"code","execution_count":null,"id":"a44f1126-7897-4cd8-bdce-b804948df69d","metadata":{"id":"a44f1126-7897-4cd8-bdce-b804948df69d"},"outputs":[],"source":["### START CODE HERE ###\n","\n","\n","###### END CODE HERE"]},{"cell_type":"code","execution_count":null,"id":"022e2c92-7a62-49f9-aae2-790c5eae36e4","metadata":{"tags":[],"id":"022e2c92-7a62-49f9-aae2-790c5eae36e4"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(##############)\n","plt.plot(#############)\n","plt.title('Loss vs. epochs, batch=' +str(batch_size))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"058a1cdc-e6ec-4fca-b87f-29a161f9986d","metadata":{"id":"058a1cdc-e6ec-4fca-b87f-29a161f9986d"},"source":["- **Diagnose this curve w.r.t learning rate**:\n","    - ANSWER:\n","- Change learning rate to make this curve better"]},{"cell_type":"code","execution_count":null,"id":"74b641cb-d7b1-4806-a4f9-216006e06e77","metadata":{"id":"74b641cb-d7b1-4806-a4f9-216006e06e77"},"outputs":[],"source":["### Your answer"]},{"cell_type":"code","execution_count":null,"id":"ab103d0c-50ad-4265-b8ee-1b32f482db95","metadata":{"id":"ab103d0c-50ad-4265-b8ee-1b32f482db95"},"outputs":[],"source":["def build_model():\n","\n","    ##### YOUR CODE HERE\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"6986d23b-8cd6-4fd5-9311-ab5c28400315","metadata":{"id":"6986d23b-8cd6-4fd5-9311-ab5c28400315"},"outputs":[],"source":["batch_size = 1\n","history = fit_model(X_train, y_train, X_test, y_test, batch_size)\n","plt.figure(figsize=(10,5))\n","plt.plot(#################)\n","plt.plot(################)\n","plt.title('Loss vs. epochs, batch=' +str(batch_size))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"ca24463c-6d6d-4dce-b365-1e0a10cbb6b9","metadata":{"tags":[],"id":"ca24463c-6d6d-4dce-b365-1e0a10cbb6b9"},"source":["##### Size =  16"]},{"cell_type":"markdown","id":"c9d3f83a-edf8-4f61-a0fe-db3261ace6ab","metadata":{"id":"c9d3f83a-edf8-4f61-a0fe-db3261ace6ab"},"source":["10. **Retrain for 16 batch size**"]},{"cell_type":"code","execution_count":null,"id":"589fd6d3-6c4d-48ed-b5db-9e3d931725d0","metadata":{"id":"589fd6d3-6c4d-48ed-b5db-9e3d931725d0"},"outputs":[],"source":["### START CODE HERE ###\n","\n","\n","###### END CODE HERE"]},{"cell_type":"code","execution_count":null,"id":"be8533d7-34a0-4072-a9f3-600ddb9e58df","metadata":{"tags":[],"id":"be8533d7-34a0-4072-a9f3-600ddb9e58df"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(###########)\n","plt.plot(###########)\n","plt.title('Loss vs. epochs, batch=' +str(batch_size))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"a7c4cd41-4aa3-4a8e-a6bb-3c7862c25787","metadata":{"id":"a7c4cd41-4aa3-4a8e-a6bb-3c7862c25787"},"source":["##### Size =  128"]},{"cell_type":"markdown","id":"55ae315d-ccfd-42aa-9e02-b2ba042756ec","metadata":{"id":"55ae315d-ccfd-42aa-9e02-b2ba042756ec"},"source":["10. **Retrain for 128 batch size**"]},{"cell_type":"code","execution_count":null,"id":"220b57cc-c220-424f-b3df-25eff109f718","metadata":{"id":"220b57cc-c220-424f-b3df-25eff109f718"},"outputs":[],"source":["### START CODE HERE ###\n","\n","\n","###### END CODE HERE"]},{"cell_type":"code","execution_count":null,"id":"7e9a46f6-a1ce-4922-b48f-3c6f54ae4515","metadata":{"tags":[],"id":"7e9a46f6-a1ce-4922-b48f-3c6f54ae4515"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(############)\n","plt.plot(############)\n","plt.title('Loss vs. epochs, batch=' +str(batch_size))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"c0c037e6-a554-454d-8d87-e009aefd826c","metadata":{"id":"c0c037e6-a554-454d-8d87-e009aefd826c"},"source":["##### Size =  Data"]},{"cell_type":"markdown","id":"f067cc9d-4c8e-434d-970e-5395914df5f1","metadata":{"id":"f067cc9d-4c8e-434d-970e-5395914df5f1"},"source":["10. **Retrain for length of data batch size**"]},{"cell_type":"code","execution_count":null,"id":"c6bcbb54-5017-4030-97a8-8a70bf2204ce","metadata":{"id":"c6bcbb54-5017-4030-97a8-8a70bf2204ce"},"outputs":[],"source":["### START CODE HERE ###\n","\n","\n","###### END CODE HERE"]},{"cell_type":"code","execution_count":null,"id":"eca79f36-f9b9-444d-950b-5c3d55dfc307","metadata":{"tags":[],"id":"eca79f36-f9b9-444d-950b-5c3d55dfc307"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(################)\n","plt.plot(###############)\n","plt.title('Loss vs. epochs, batch=' +str(batch_size))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"d58d23e3-89c6-4596-80d3-33b98d895eb1","metadata":{"id":"d58d23e3-89c6-4596-80d3-33b98d895eb1"},"source":["10. **What effect does changing batch size have on learning in terms of convergence and fluctuations?**\n","\n","**Answer:**\n","\n"]},{"cell_type":"markdown","id":"ac0b0cdb-78c2-4239-bbb9-4bea511b52b1","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"ac0b0cdb-78c2-4239-bbb9-4bea511b52b1"},"source":["#### Learning Rate"]},{"cell_type":"markdown","id":"c7be280d-068a-4695-b716-c1ef97d4d6f4","metadata":{"id":"c7be280d-068a-4695-b716-c1ef97d4d6f4"},"source":["batch size 64"]},{"cell_type":"markdown","id":"2970cd51-b911-455a-b6c3-17f477415759","metadata":{"id":"2970cd51-b911-455a-b6c3-17f477415759"},"source":["- **Use same code for build_model above, add an argument learning rate to change learning rate of optmizer**"]},{"cell_type":"code","execution_count":null,"id":"939ebb69-e520-4d68-9fa7-ce00f0257629","metadata":{"id":"939ebb69-e520-4d68-9fa7-ce00f0257629"},"outputs":[],"source":["def build_model(lr):\n","\n","    ##### YOUR CODE HERE\n","\n","    return model"]},{"cell_type":"markdown","id":"72f1ebf6-5bcb-4e2b-bee7-3eb282fd88b1","metadata":{"id":"72f1ebf6-5bcb-4e2b-bee7-3eb282fd88b1"},"source":["##### LR =  0.00001"]},{"cell_type":"markdown","id":"739eab80-d78f-4ca3-9992-42096088cf7a","metadata":{"id":"739eab80-d78f-4ca3-9992-42096088cf7a"},"source":["10. **Retrain for 0.00001 LR**"]},{"cell_type":"code","execution_count":null,"id":"fb2ad280-97f1-4c16-9aad-d99c96780472","metadata":{"tags":[],"id":"fb2ad280-97f1-4c16-9aad-d99c96780472"},"outputs":[],"source":["### START CODE HERE ### ()\n"]},{"cell_type":"code","execution_count":null,"id":"6f5e459c-4f1e-4bf3-88d7-3a7af2f789eb","metadata":{"id":"6f5e459c-4f1e-4bf3-88d7-3a7af2f789eb"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(############)\n","plt.plot(############)\n","plt.title('Loss vs. epochs, lr=' +str(lr))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"467481cd-d773-479b-837b-94b0cd4c056a","metadata":{"id":"467481cd-d773-479b-837b-94b0cd4c056a"},"source":["##### LR =  0.001"]},{"cell_type":"markdown","id":"bd885be3-5ac4-4bad-80eb-7a9e1850a3e5","metadata":{"id":"bd885be3-5ac4-4bad-80eb-7a9e1850a3e5"},"source":["10. **Retrain for 0.001 LR**"]},{"cell_type":"code","execution_count":null,"id":"afa0d071-55ed-42e5-bd40-ef410c33128f","metadata":{"tags":[],"id":"afa0d071-55ed-42e5-bd40-ef410c33128f"},"outputs":[],"source":["### START CODE HERE ### ()\n"]},{"cell_type":"code","execution_count":null,"id":"53bf0abf-a145-4081-aa0b-e4ffe73c9357","metadata":{"id":"53bf0abf-a145-4081-aa0b-e4ffe73c9357"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(#################)\n","plt.plot(#################)\n","plt.title('Loss vs. epochs, lr=' +str(lr))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"9c476d8f-d9d5-49e9-bcb1-c47f3505d456","metadata":{"id":"9c476d8f-d9d5-49e9-bcb1-c47f3505d456"},"source":["##### LR =  0.1"]},{"cell_type":"markdown","id":"5538b4df-7c6b-4d4d-9cbd-e7b14e57a8d6","metadata":{"id":"5538b4df-7c6b-4d4d-9cbd-e7b14e57a8d6"},"source":["10. **Retrain for 0.1 LR**"]},{"cell_type":"code","execution_count":null,"id":"2c320fde-e42b-4d0a-941c-86b2c5817c66","metadata":{"tags":[],"id":"2c320fde-e42b-4d0a-941c-86b2c5817c66"},"outputs":[],"source":["### START CODE HERE ### ()\n"]},{"cell_type":"code","execution_count":null,"id":"abadecd9-9a79-45c0-8540-bf16d8b4bc27","metadata":{"id":"abadecd9-9a79-45c0-8540-bf16d8b4bc27"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.plot(###################)\n","plt.plot(###################)\n","plt.title('Loss vs. epochs, lr=' +str(lr))\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc='upper right')\n","plt.show()"]},{"cell_type":"markdown","id":"1c665798-8db4-4b7f-a242-b34f3d0d317d","metadata":{"id":"1c665798-8db4-4b7f-a242-b34f3d0d317d"},"source":["10. **What effect does changing learning rate have on learning?**\n","\n","\n","\n"]},{"cell_type":"markdown","id":"ddce9e2f-6bf9-4b68-a2b7-4de22effc3a2","metadata":{"id":"ddce9e2f-6bf9-4b68-a2b7-4de22effc3a2"},"source":["**Answer:**"]},{"cell_type":"markdown","id":"c51ddaa1-aee4-4f8a-9f9c-07a1ec4c14fd","metadata":{"id":"c51ddaa1-aee4-4f8a-9f9c-07a1ec4c14fd"},"source":["**Questions**\n","\n","1. Mention 1 advantage of a computational graph?"]},{"cell_type":"markdown","id":"46082e1b-e16f-4d24-bdff-50afe54981e5","metadata":{"id":"46082e1b-e16f-4d24-bdff-50afe54981e5"},"source":["**Answer:**"]},{"cell_type":"markdown","id":"775c5c01-58e7-4af9-8d11-c2bcb2d3acee","metadata":{"id":"775c5c01-58e7-4af9-8d11-c2bcb2d3acee"},"source":["2. Mention two ways to input data for training using TensorFlow if the data resides on disk"]},{"cell_type":"markdown","id":"8337faaf-f4cd-4c4c-9dcb-0a879b4c2cf3","metadata":{"id":"8337faaf-f4cd-4c4c-9dcb-0a879b4c2cf3"},"source":["**Answer:**\n","1. ______________\n","\n","2. ____________"]},{"cell_type":"code","execution_count":null,"id":"8d87013f-708a-4aad-be50-e7c19d7422dd","metadata":{"id":"8d87013f-708a-4aad-be50-e7c19d7422dd"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}